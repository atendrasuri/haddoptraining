/**
 * 
 */
package com.sapient.spark;

import java.util.Arrays;

import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.sql.SparkSession;



/**
 * @author aku375
 *
 */
public class WordCount {

	/**
	 * @param args
	 */
	public static void main(String[] args) {
		
        SparkSession spark = SparkSession.builder().master("local").appName("JavaWordCount").getOrCreate();

        SparkContext jsc = spark.sparkContext();

        JavaRDD<String> baseRDD = spark.read().textFile(SparkUtil.getActualPath(SparkWordCount.class, "WordCount")).javaRDD();

        List<Tuple2<String, Integer>> wordCountList = baseRDD
                                        .flatMap(lines -> Arrays.asList(lines.split(" ")).iterator()).mapToPair(word -> new Tuple2<>(word, 1))
                                        .reduceByKey((x, y) -> x + y).collect();

        wordCountList.stream().forEach(wordCount -> System.out.println(wordCount._1() + "---->" + wordCount._2()));
        
        Thread.sleep(50000000);
        spark.close();


	  }

	

}
